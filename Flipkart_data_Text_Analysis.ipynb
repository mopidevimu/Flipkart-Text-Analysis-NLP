{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flipkart data Text Analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQSxg8lMCZx",
        "colab_type": "text"
      },
      "source": [
        "**Importing Libraries**\n",
        "--\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmwxvU2vCWtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk  \n",
        "import random  \n",
        "import string \n",
        "from sklearn.base import TransformerMixin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQnEnZUEMaMg",
        "colab_type": "text"
      },
      "source": [
        "**Reading Data From CSV File**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjkVXMBICiRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('flipkart.csv', low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpzGJTIjMkBg",
        "colab_type": "text"
      },
      "source": [
        "**Printing DataFrame**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFluKH4CvMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3YhZoMVM6o9",
        "colab_type": "text"
      },
      "source": [
        "**Cleaning Messy Column Names**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P_fi4TMC8yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns = data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNUxKJYlNNf4",
        "colab_type": "text"
      },
      "source": [
        "**Creating a NEW dataframe from acutal dataframe by selecting specfic columns**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGpQMJfDo8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = data[['brand','description']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCHC8a2jNemb",
        "colab_type": "text"
      },
      "source": [
        "**Displaying the NEW dataframe**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S65x5q6DvKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHOsn6i3Ih6r",
        "colab_type": "text"
      },
      "source": [
        "**Removing Special Characters from Dataframe**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuXMbQwvGahV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replacing dataFrame in another Variable.\n",
        "df_spsl = df\n",
        "\n",
        "# Declaration all Sorts of Special Characters in the form of List.\n",
        "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \"//\", \"%*\", \":/\", \".;\", \"Ø\", \"§\"]\n",
        "\n",
        "# Checking\n",
        "for c in spec_chars:\n",
        "    c1 = \"\\\\\" + c\n",
        "    # Replacing With SPACE\"\".\n",
        "    df_spsl=df_spsl.replace(c1,\"\", regex=True)\n",
        "\n",
        "# Replacing DataFrame with no Special characters.\n",
        "df1 = df_spsl\n",
        "display(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO0TIN84NxLV",
        "colab_type": "text"
      },
      "source": [
        "**Counting NAN values in dataframe**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YTTDdm9N5K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGXDvk-VOR7-",
        "colab_type": "text"
      },
      "source": [
        "**Creating a custom class to Impute Null values using TransformerMixin**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHqFKMDzIVoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataFrameImputer(TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Impute missing values.\n",
        "\n",
        "        Columns of dtype object are imputed with the most frequent value \n",
        "        in column.\n",
        "\n",
        "        Columns of other types are imputed with mean of column.\n",
        "\n",
        "        \"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
        "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
        "            index=X.columns)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X.fillna(self.fill)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABZt__7dOgn1",
        "colab_type": "text"
      },
      "source": [
        "**Applying Imputation on a Dataframe**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsEfTHYEJUBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = DataFrameImputer().fit_transform(df1)\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB4H4w4XOub5",
        "colab_type": "text"
      },
      "source": [
        "**Checking for NA values again**\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGkAeKzFJmlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isna().sum()\n",
        "df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjRa11VPE6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning the texts\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "rows = df.shape[0]\n",
        "for i in range(0, rows):\n",
        "    des = re.sub('[^a-zA-Z]', ' ', df['description'][i])\n",
        "    des = des.lower()\n",
        "    des = des.split()\n",
        "    ps = PorterStemmer()\n",
        "    des = [ps.stem(word) for word in des if not word in set(stopwords.words('english'))]\n",
        "    des = ' '.join(des)\n",
        "    corpus.append(des)\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 10)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmE-VJRdRmBF",
        "colab_type": "text"
      },
      "source": [
        "** With Keras **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJWRZUUOLJcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from nltk import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "\n",
        "# download('stopwords')\n",
        "\n",
        "df = pd.read_csv(\"final_text_db.csv\", encoding='utf-8')\n",
        "\n",
        "class Preprocess():\n",
        "\tdef __init__(self, text, sw=stopwords.words('english'), lower=True, stem = True):\n",
        "\n",
        "\t\tif not (type(text)==pd.core.series.Series):\n",
        "\t\t\ttext = pd.Series(text)\n",
        "\n",
        "\t\tself.text = text\n",
        "\t\tself.sw = sw\n",
        "\t\tself.lower = lower\n",
        "\t\tself.stem = stem\n",
        "\n",
        "\n",
        "\tdef clean_text(self):\n",
        "\t\tdef stem(word_list):\n",
        "\t\t\treturn map(lambda x: PorterStemmer().stem(x), word_list)\n",
        "\n",
        "\t\tdef remove_sw(word_list):\n",
        "\t\t\tkeep = []\n",
        "\t\t\tfor word in word_list:\n",
        "\t\t\t\tif not word in self.sw:\n",
        "\t\t\t\t\tkeep.append(word)\n",
        "\t\t\treturn keep\n",
        "\n",
        "\t\tif self.lower:\n",
        "\t\t\tself.text = self.text.str.lower()\n",
        "\n",
        "\t\tself.text = self.text.apply(lambda x: x.split())\n",
        "\t\t\n",
        "\t\tif self.stem: self.text = self.text.apply(stem)\n",
        "\t\tif self.sw: self.text = self.text.apply(remove_sw)\n",
        "\n",
        "\t\tself.text = self.text.apply(lambda x: ' '.join(x))\n",
        "\t\tself.vectorizer = TfidfVectorizer()\n",
        "\t\tself.df_dense = self.vectorizer.fit_transform(self.text)\n",
        "\n",
        "\tdef array(self, onehot=1):\n",
        "\t\tarray = self.df_dense.toarray().copy()\n",
        "\t\tif onehot:\n",
        "\t\t\tarray[array>0] = 1\n",
        "\t\treturn array\n",
        "\n",
        "\tdef make_df(self,onehot=1):\n",
        "\t\tdf = pd.DataFrame(columns=self.vectorizer.get_feature_names(),\n",
        "\t\t\t\t\t\t\tdata = self.array(onehot))\n",
        "\t\tdf['Text'] = self.text\n",
        "\t\tdf = df[['Text']+list(df.columns[:-1])]\n",
        "\t\treturn df\n",
        "\n",
        "docs = Preprocess(df.QUOTE)\n",
        "docs.clean_text()\n",
        "text_df = docs.make_df(onehot=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}